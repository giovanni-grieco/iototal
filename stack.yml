---
# Namespace declaration for the big data components
apiVersion: v1
kind: Namespace
metadata:
  name: big-data

---
# HDFS - Hadoop Distributed File System Configuration
# Persistent Volumes for HDFS
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: namenode-data
  namespace: big-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: datanode-data
  namespace: big-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
---
# HDFS NameNode Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hdfs-namenode
  namespace: big-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hdfs-namenode
  template:
    metadata:
      labels:
        app: hdfs-namenode
    spec:
      containers:
      - name: namenode
        image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
        ports:
        - containerPort: 9870
          name: web
        - containerPort: 9000
          name: fs
        env:
        - name: CLUSTER_NAME
          value: "hadoop-cluster"
        - name: CORE_CONF_fs_defaultFS
          value: "hdfs://hdfs-namenode:9000"
        - name: CORE_CONF_hadoop_http_staticuser_user
          value: "root"
        - name: HDFS_CONF_dfs_webhdfs_enabled
          value: "true"
        - name: HDFS_CONF_dfs_permissions_enabled
          value: "false"
        volumeMounts:
        - name: namenode-data
          mountPath: /hadoop/dfs/name
      volumes:
      - name: namenode-data
        persistentVolumeClaim:
          claimName: namenode-data
---
# HDFS NameNode Service
apiVersion: v1
kind: Service
metadata:
  name: hdfs-namenode
  namespace: big-data
spec:
  selector:
    app: hdfs-namenode
  ports:
  - name: web
    port: 9870
    targetPort: 9870
  - name: fs
    port: 9000
    targetPort: 9000
---
# HDFS DataNode Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hdfs-datanode
  namespace: big-data
spec:
  replicas: 3
  selector:
    matchLabels:
      app: hdfs-datanode
  template:
    metadata:
      labels:
        app: hdfs-datanode
    spec:
      containers:
      - name: datanode
        image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
        ports:
        - containerPort: 9864
          name: web
        - containerPort: 9866
          name: data
        env:
        - name: SERVICE_PRECONDITION
          value: "hdfs-namenode:9000"
        - name: CORE_CONF_fs_defaultFS
          value: "hdfs://hdfs-namenode:9000"
        - name: HDFS_CONF_dfs_datanode_data_dir
          value: "/hadoop/dfs/data"
        volumeMounts:
        - name: datanode-data
          mountPath: /hadoop/dfs/data
      volumes:
      - name: datanode-data
        persistentVolumeClaim:
          claimName: datanode-data
---
# HDFS DataNode Service
apiVersion: v1
kind: Service
metadata:
  name: hdfs-datanode
  namespace: big-data
spec:
  selector:
    app: hdfs-datanode
  ports:
  - name: web
    port: 9864
    targetPort: 9864
  - name: data
    port: 9866
    targetPort: 9866
---
# Kafka Configuration
# ZooKeeper Deployment for Kafka
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
  namespace: big-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
      - name: zookeeper
        image: zookeeper
        ports:
        - containerPort: 2181
        env:
        - name: ZOOKEEPER_CLIENT_PORT
          value: "2181"
        - name: ZOOKEEPER_TICK_TIME
          value: "2000"
        readinessProbe:
          tcpSocket:
            port: 2181
          initialDelaySeconds: 10
          periodSeconds: 10
---
# ZooKeeper Service
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: big-data
spec:
  selector:
    app: zookeeper
  ports:
  - name: client
    port: 2181
    targetPort: 2181
---
# Kafka Deployment
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: big-data
spec:
  serviceName: "kafka"
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
      - name: kafka
        image: apache/kafka
        ports:
        - containerPort: 9092
          name: plaintext
        env:
        - name: KAFKA_BROKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "zookeeper:2181"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://$(POD_NAME).kafka.big-data.svc.cluster.local:9092"
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "PLAINTEXT:PLAINTEXT"
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"
        - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
          value: "true"
        - name: KAFKA_LOG_DIRS
          value: "/kafka/logs"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: kafka-data
          mountPath: /kafka/logs
  volumeClaimTemplates:
  - metadata:
      name: kafka-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 30Gi
---
# Kafka Service (Headless service for StatefulSet)
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: big-data
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
  - port: 9092
    targetPort: 9092
    name: plaintext
---
# Kafka Client Service (for external connections)
apiVersion: v1
kind: Service
metadata:
  name: kafka-client
  namespace: big-data
spec:
  selector:
    app: kafka
  ports:
  - port: 9092
    targetPort: 9092
    name: plaintext
  type: ClusterIP # Change to NodePort or LoadBalancer if external access needed
---
# Spark Configuration
# ConfigMap for Spark configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-config
  namespace: big-data
data:
  spark-defaults.conf: |
    spark.master                     k8s://https://kubernetes.default.svc.cluster.local:443
    spark.kubernetes.namespace       big-data
    spark.eventLog.enabled           true
    spark.eventLog.dir               hdfs://hdfs-namenode:9000/spark-logs
    spark.hadoop.fs.defaultFS        hdfs://hdfs-namenode:9000
    spark.driver.extraJavaOptions    -Divy.cache.dir=/tmp -Divy.home=/tmp
    spark.executor.instances         2
    spark.kubernetes.container.image bitnami/spark:3.3.0
---
# Spark Master Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-master
  namespace: big-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-master
  template:
    metadata:
      labels:
        app: spark-master
    spec:
      containers:
      - name: spark-master
        image: bitnami/spark:3.3.0
        command: ["/bin/bash", "-c"]
        args:
        - >
          mkdir -p /opt/spark/logs &&
          /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
        ports:
        - containerPort: 7077
          name: master
        - containerPort: 8080
          name: webui
        env:
        - name: SPARK_MASTER_HOST
          value: spark-master
        - name: SPARK_MASTER_PORT
          value: "7077"
        - name: SPARK_MASTER_WEBUI_PORT
          value: "8080"
        - name: SPARK_MASTER_LOG_DIR
          value: /opt/spark/logs
        volumeMounts:
        - name: spark-logs
          mountPath: /opt/spark/logs
      volumes:
      - name: spark-logs
        emptyDir: {}
---
# Spark Master Service
apiVersion: v1
kind: Service
metadata:
  name: spark-master
  namespace: big-data
spec:
  selector:
    app: spark-master
  ports:
  - name: master
    port: 7077
    targetPort: 7077
  - name: webui
    port: 8080
    targetPort: 8080
---
# Spark Worker Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-worker
  namespace: big-data
spec:
  replicas: 3
  selector:
    matchLabels:
      app: spark-worker
  template:
    metadata:
      labels:
        app: spark-worker
    spec:
      containers:
      - name: spark-worker
        image: bitnami/spark:3.3.0
        command: ["/bin/bash", "-c"]
        args:
        - >
          mkdir -p /opt/spark/logs &&
          /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
        ports:
        - containerPort: 8081
          name: webui
        env:
        - name: SPARK_WORKER_CORES
          value: "2"
        - name: SPARK_WORKER_MEMORY
          value: "4g"
        - name: SPARK_WORKER_WEBUI_PORT
          value: "8081"
        - name: SPARK_WORKER_LOG_DIR
          value: /opt/spark/logs
        volumeMounts:
        - name: spark-logs
          mountPath: /opt/spark/logs
      volumes:
      - name: spark-logs
        emptyDir: {}
---
# Service Account for Spark
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark
  namespace: big-data
---
# Role for Spark to manage executors
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-role
  namespace: big-data
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
# RoleBinding for Spark
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-role-binding
  namespace: big-data
subjects:
- kind: ServiceAccount
  name: spark
  namespace: big-data
roleRef:
  kind: Role
  name: spark-role
  apiGroup: rbac.authorization.k8s.io
---
# Spark History Server Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-history-server
  namespace: big-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-history-server
  template:
    metadata:
      labels:
        app: spark-history-server
    spec:
      containers:
      - name: spark-history-server
        image: bitnami/spark:3.3.0
        command: ["/bin/bash", "-c"]
        args:
        - >
          mkdir -p /tmp/spark-events &&
          /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
        ports:
        - containerPort: 18080
          name: webui
        env:
        - name: SPARK_HISTORY_OPTS
          value: "-Dspark.history.fs.logDirectory=hdfs://hdfs-namenode:9000/spark-logs -Dspark.history.fs.update.interval=10s"
---
# Spark History Server Service
apiVersion: v1
kind: Service
metadata:
  name: spark-history-server
  namespace: big-data
spec:
  selector:
    app: spark-history-server
  ports:
  - name: webui
    port: 18080
    targetPort: 18080
---
# Ingress for Web UIs
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: big-data-ingress
  namespace: big-data
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: hdfs.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: hdfs-namenode
            port:
              number: 9870
  - host: spark-master.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: spark-master
            port:
              number: 8080
  - host: spark-history.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: spark-history-server
            port:
              number: 18080